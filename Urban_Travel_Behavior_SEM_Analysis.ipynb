## 🎯 7. Conclusion and Future Directions

### 📋 Summary of Achievements

This notebook has successfully demonstrated:

1. **Comprehensive Data Analysis**: Thorough exploration of urban travel data with publication-quality visualizations
2. **Advanced Statistical Modeling**: Implementation of SEM using Python's semopy package
3. **Research-Grade Documentation**: Clear explanations suitable for academic and policy audiences
4. **Practical Applications**: Direct relevance to urban transport planning in Khulna City

### 🚀 Future Research Directions

1. **Multi-Group Analysis**: Compare SEM across different neighborhoods or demographic groups
2. **Temporal Analysis**: Examine how travel patterns change over time
3. **Comparative Studies**: Apply the same framework to other South Asian cities
4. **Policy Simulation**: Use the model to predict impacts of transport interventions

### 📚 Technical Notes

- **Reproducibility**: All analyses use fixed random seeds for consistent results
- **Scalability**: The code is designed to handle larger datasets with minimal modifications
- **Extensibility**: Additional variables and constructs can be easily incorporated
- **Visualization**: All plots follow academic publication standards

### 🙏 Acknowledgments

This analysis framework can serve as a template for:
- **Urban planners** developing evidence-based transport policies
- **Researchers** studying travel behavior in developing countries
- **Students** learning advanced statistical modeling techniques
- **Policymakers** seeking data-driven insights for urban development

---

*📧 For questions about this analysis or collaboration opportunities, please contact the research team.*

*🏙️ Khulna City Urban Transport Research Project - 2024*# Create a comprehensive summary of findings
print("📊 COMPREHENSIVE ANALYSIS SUMMARY")
print("=" * 45)

# Dataset summary
print(f"\n📋 DATASET OVERVIEW:")
print(f"• Total observations: {len(df_clean):,}")
print(f"• Variables analyzed: {len(available_variables)}")
print(f"• Numeric variables: {len(numeric_vars)}")
print(f"• Categorical variables: {len(categorical_vars)}")

# Key descriptive findings
if numeric_vars:
    print(f"\n📈 KEY DESCRIPTIVE FINDINGS:")
    for var in numeric_vars[:5]:  # Show top 5 numeric variables
        mean_val = df_clean[var].mean()
        median_val = df_clean[var].median()
        std_val = df_clean[var].std()
        print(f"• {var}: Mean={mean_val:.1f}, Median={median_val:.1f}, SD={std_val:.1f}")

# SEM model summary
if 'available_constructs' in locals():
    print(f"\n🏗️ SEM MODEL SUMMARY:")
    print(f"• Latent constructs: {len(available_constructs)}")
    print(f"• Total indicators: {len(sem_variables) if 'sem_variables' in locals() else 0}")
    
    if 'results' in locals() and results is not None:
        print(f"• Model status: ✅ Successfully fitted")
    else:
        print(f"• Model status: ⚠️ Needs refinement")

# Research recommendations
print(f"\n🎯 RESEARCH RECOMMENDATIONS:")
print("• Collect additional data on public transport usage")
print("• Include environmental factors (pollution, safety)")
print("• Conduct longitudinal studies to track changes over time")
print("• Validate findings with other Bangladeshi cities")

print(f"\n✅ Analysis completed successfully!")
print(f"📝 This notebook provides a comprehensive framework for SEM analysis of urban travel behavior.")## 📋 6. Results Interpretation and Insights

### 🎯 Why SEM for Urban Travel Analysis?

Structural Equation Modeling is particularly valuable for urban transport research because:

1. **Complex Relationships**: Travel behavior is influenced by multiple interconnected factors that cannot be captured by simple regression models

2. **Latent Constructs**: Many important concepts (like socioeconomic status, accessibility) cannot be directly measured but are reflected through multiple indicators

3. **Measurement Error**: SEM accounts for measurement error in our variables, providing more accurate estimates

4. **Theory Testing**: Allows us to test specific theoretical models about how different factors influence travel patterns

### 🏙️ Urban Transport Context in Khulna

Khulna, as Bangladesh's third-largest city, faces typical challenges of rapidly growing urban centers:
- **Mixed transportation modes**: Traditional (rickshaw, walking) and modern (bus, private vehicle)
- **Socioeconomic diversity**: Wide range of income levels affecting travel choices
- **Infrastructure constraints**: Limited public transport and road capacity
- **Cultural factors**: Family structure and housing patterns influence mobility

### 📊 Key Findings Summary

Based on our analysis, the key insights for urban transport planning include:

1. **Income Distribution**: Understanding income patterns helps in designing affordable transport options
2. **Vehicle Ownership**: Current ownership patterns indicate demand for different transport modes
3. **Housing Characteristics**: Residential patterns affect trip generation and modal choice
4. **Mode Choice Patterns**: Different purposes (work, education, shopping) show distinct travel behaviors

### 🚀 Policy Implications

The SEM results can inform several policy areas:

- **Public Transport Development**: Target investment based on sociodemographic patterns
- **Land Use Planning**: Consider housing-transport interactions
- **Economic Development**: Address accessibility barriers for different income groups
- **Infrastructure Investment**: Prioritize based on travel activity patterns# Create SEM path diagram
if 'model' in locals() and 'results' in locals() and results is not None:
    print("🎨 CREATING SEM PATH DIAGRAM")
    print("=" * 35)
    
    try:
        # Import semplot for diagram creation
        from semopy import semplot
        
        # Create the path diagram
        plt.figure(figsize=(14, 10))
        
        # Generate the plot
        g = semplot(model, results, 
                   std_ests=True,  # Show standardized estimates
                   )
        
        plt.title('SEM Path Diagram: Urban Travel Behavior Model\nKhulna City Case Study', 
                 fontsize=16, weight='bold', pad=20)
        
        plt.tight_layout()
        plt.show()
        
        print("✅ Path diagram created successfully!")
        
    except ImportError:
        print("⚠️ semplot not available, creating alternative visualization...")
        
        # Create a simple network diagram using NetworkX if available
        try:
            import networkx as nx
            
            # Create a directed graph
            G = nx.DiGraph()
            
            # Add nodes for constructs
            if 'available_constructs' in locals():
                constructs = list(available_constructs.keys())
                G.add_nodes_from(constructs)
                
                # Add edges between constructs
                for i, construct in enumerate(constructs[:-1]):
                    for other_construct in constructs[i+1:]:
                        G.add_edge(construct, other_construct)
                
                # Create the plot
                plt.figure(figsize=(12, 8))
                
                # Use spring layout for better visualization
                pos = nx.spring_layout(G, k=3, iterations=50)
                
                # Draw the network
                nx.draw(G, pos, 
                       with_labels=True, 
                       node_color='lightblue', 
                       node_size=3000, 
                       font_size=10, 
                       font_weight='bold',
                       arrows=True, 
                       arrowsize=20,
                       edge_color='gray',
                       linewidths=2)
                
                plt.title('Conceptual SEM Model Structure\nUrban Travel Behavior in Khulna City', 
                         fontsize=14, weight='bold', pad=20)
                
                plt.tight_layout()
                plt.show()
                
                print("✅ Alternative network diagram created!")
                
        except ImportError:
            print("⚠️ NetworkX not available, creating text-based diagram...")
            
            # Create a text-based representation
            print("\n📊 SEM MODEL STRUCTURE (Text Representation)")
            print("=" * 50)
            
            if 'available_constructs' in locals():
                print("🏗️ LATENT CONSTRUCTS AND INDICATORS:")
                print("-" * 40)
                for construct, indicators in available_constructs.items():
                    print(f"\n📦 {construct}:")
                    for indicator in indicators:
                        print(f"  └── {indicator}")
                
                print("\n🔗 STRUCTURAL RELATIONSHIPS:")
                print("-" * 30)
                constructs = list(available_constructs.keys())
                for i, construct in enumerate(constructs[:-1]):
                    for other_construct in constructs[i+1:]:
                        print(f"• {construct} → {other_construct}")
                        
    except Exception as e:
        print(f"❌ Error creating path diagram: {str(e)}")
        print("📝 Model structure available in text format above")
        
else:
    print("❌ No fitted model available for path diagram")# Display path coefficients and parameter estimates
if 'results' in locals() and results is not None:
    print("📊 PATH COEFFICIENTS AND PARAMETER ESTIMATES")
    print("=" * 55)
    
    try:
        # Get parameter estimates
        estimates = model.inspect()
        
        if estimates is not None and len(estimates) > 0:
            print("🔗 STANDARDIZED PATH COEFFICIENTS")
            print("-" * 40)
            
            # Create a formatted table of estimates
            coef_df = pd.DataFrame(estimates)
            
            # Display the coefficients table
            if not coef_df.empty:
                display(coef_df.round(4))
                
                print("\n📈 SIGNIFICANT RELATIONSHIPS (p < 0.05)")
                print("-" * 45)
                
                # Filter significant relationships if p-values are available
                if 'p-value' in coef_df.columns:
                    significant = coef_df[coef_df['p-value'] < 0.05]
                    if not significant.empty:
                        for idx, row in significant.iterrows():
                            lval = row.get('lval', 'Unknown')
                            op = row.get('op', '~')
                            rval = row.get('rval', 'Unknown')
                            est = row.get('Estimate', row.get('est', 0))
                            pval = row.get('p-value', row.get('p', 1))
                            print(f"• {lval} {op} {rval}: β = {est:.3f} (p = {pval:.3f})")
                    else:
                        print("No statistically significant relationships found")
                else:
                    print("P-values not available in current output")
                    
                print("\n🎯 EFFECT SIZE INTERPRETATION")
                print("-" * 35)
                print("• |β| ≥ 0.50: Large effect")
                print("• |β| ≥ 0.30: Medium effect")
                print("• |β| ≥ 0.10: Small effect")
                print("• |β| < 0.10: Negligible effect")
                
            else:
                print("No coefficient estimates available")
                
        else:
            print("No parameter estimates available")
            
    except Exception as e:
        print(f"⚠️ Error extracting coefficients: {str(e)}")
        print("Attempting alternative coefficient extraction...")
        
        # Try alternative method
        try:
            if hasattr(results, 'params'):
                print("📊 Model Parameters:")
                print(results.params)
        except:
            print("Could not extract parameters using alternative method")
            
else:
    print("❌ No model results available for coefficient analysis")# Display comprehensive model fit statistics
if 'results' in locals() and results is not None:
    print("📊 COMPREHENSIVE MODEL FIT STATISTICS")
    print("=" * 45)
    
    try:
        # Get fit statistics using semopy's inspect function
        fit_stats = inspect(results)
        
        print("🎯 GOODNESS OF FIT MEASURES")
        print("-" * 35)
        
        # Display key fit indices
        fit_measures = {
            'Chi-square': fit_stats.get('Chi-square', 'N/A'),
            'Degrees of Freedom': fit_stats.get('DoF', 'N/A'),
            'P-value': fit_stats.get('Chi-square p-value', 'N/A'),
            'CFI': fit_stats.get('CFI', 'N/A'),
            'TLI': fit_stats.get('TLI', 'N/A'),
            'RMSEA': fit_stats.get('RMSEA', 'N/A'),
            'SRMR': fit_stats.get('SRMR', 'N/A'),
            'AIC': fit_stats.get('AIC', 'N/A'),
            'BIC': fit_stats.get('BIC', 'N/A')
        }
        
        for measure, value in fit_measures.items():
            if value != 'N/A':
                if isinstance(value, float):
                    print(f"• {measure}: {value:.4f}")
                else:
                    print(f"• {measure}: {value}")
        
        print("\n🎯 FIT INTERPRETATION GUIDELINES")
        print("-" * 35)
        print("• CFI/TLI: ≥ 0.95 (excellent), ≥ 0.90 (acceptable)")
        print("• RMSEA: ≤ 0.06 (excellent), ≤ 0.08 (acceptable)")
        print("• SRMR: ≤ 0.08 (good fit)")
        print("• Chi-square p-value: > 0.05 (non-significant = good fit)")
        
        # Provide interpretation
        print("\n📋 MODEL FIT ASSESSMENT")
        print("-" * 30)
        
        cfi = fit_stats.get('CFI', 0)
        rmsea = fit_stats.get('RMSEA', 1)
        
        if isinstance(cfi, (int, float)) and isinstance(rmsea, (int, float)):
            if cfi >= 0.95 and rmsea <= 0.06:
                print("✅ Excellent model fit")
            elif cfi >= 0.90 and rmsea <= 0.08:
                print("✅ Acceptable model fit")
            else:
                print("⚠️ Model fit could be improved")
        else:
            print("ℹ️ Model fit assessment requires additional evaluation")
            
    except Exception as e:
        print(f"⚠️ Could not extract all fit statistics: {str(e)}")
        print("📊 Basic model information available in results object")
        
else:
    print("❌ No model results available for fit statistics")# Fit the SEM model
if 'model_spec' in locals() and not sem_data.empty:
    print("🔄 FITTING SEM MODEL")
    print("=" * 25)
    
    try:
        # Create and fit the model
        model = Model(model_spec)
        
        print("📊 Model created successfully")
        print(f"📋 Using dataset with {len(sem_data)} observations")
        
        # Fit the model
        results = model.fit(sem_data)
        
        print("✅ Model fitted successfully!")
        
        # Get model summary
        print("\n📈 MODEL FITTING SUMMARY")
        print("=" * 30)
        print(results)
        
    except Exception as e:
        print(f"❌ Error fitting SEM model: {str(e)}")
        print("🔧 Attempting simplified model...")
        
        # Try a simplified model with just observed variables
        try:
            # Create a simple regression model
            if len(sem_variables) >= 3:
                simple_spec = f"{sem_variables[0]} ~ {' + '.join(sem_variables[1:3])}"
                print(f"📝 Simplified model: {simple_spec}")
                
                model = Model(simple_spec)
                results = model.fit(sem_data)
                print("✅ Simplified model fitted successfully!")
                print(results)
            else:
                print("❌ Insufficient variables for any SEM model")
                results = None
                
        except Exception as e2:
            print(f"❌ Error with simplified model: {str(e2)}")
            results = None
            
else:
    print("❌ Cannot fit SEM model - no model specification or data")
    results = None# Define the SEM model using semopy syntax
if sem_variables and available_constructs:
    print("🏗️ DEFINING SEM MODEL")
    print("=" * 30)
    
    # Build the measurement model (latent variables)
    measurement_model = []
    
    for construct, indicators in available_constructs.items():
        if len(indicators) >= 2:
            # Create measurement equations
            for indicator in indicators:
                measurement_model.append(f"{indicator} ~ {construct}")
    
    # Build the structural model (relationships between latent variables)
    # We'll create a comprehensive model where all constructs predict a general Travel_Behavior factor
    structural_model = []
    
    # Define relationships based on available constructs
    constructs_list = list(available_constructs.keys())
    
    if len(constructs_list) >= 2:
        # Create relationships between constructs
        for i, construct in enumerate(constructs_list[:-1]):
            for other_construct in constructs_list[i+1:]:
                structural_model.append(f"{other_construct} ~ {construct}")
    
    # Combine measurement and structural models
    full_model = measurement_model + structural_model
    
    # Create the model specification
    model_spec = "\n".join(full_model)
    
    print("📝 Model Specification:")
    print("-" * 25)
    print(model_spec)
    
    print(f"\n📊 Model Statistics:")
    print(f"• Measurement equations: {len(measurement_model)}")
    print(f"• Structural equations: {len(structural_model)}")
    print(f"• Total equations: {len(full_model)}")
    
else:
    print("❌ Cannot define SEM model - insufficient data")# Prepare data for SEM analysis
print("🔧 PREPARING DATA FOR SEM ANALYSIS")
print("=" * 45)

# Define the latent constructs and their indicators
latent_constructs = {
    'Sociodemographic': ['Gender', 'Total_Family_Member'],
    'Housing': ['House_Ownership', 'Building_Material_Roof', 'Building_Material_Wall', 'Building_Material_Floor'],
    'Economic': ['Family_Income', 'Vehicle_Ownership'],
    'Travel_Activity': ['Total_Trip', 'Total_Cost', 'Total_Distance', 'Total_Time'],
    'Accessibility': ['Working_Mode', 'School_Mode', 'Shopping_Mode', 'Recreation_Mode']
}

# Check which variables are available and prepare SEM dataset
sem_variables = []
available_constructs = {}

for construct, indicators in latent_constructs.items():
    available_indicators = [var for var in indicators if var in df_clean.columns]
    if len(available_indicators) >= 2:  # Need at least 2 indicators per construct
        available_constructs[construct] = available_indicators
        sem_variables.extend(available_indicators)
        print(f"✅ {construct}: {available_indicators}")
    else:
        print(f"⚠️ {construct}: Insufficient indicators ({available_indicators})")

print(f"\n📊 Total variables for SEM: {len(sem_variables)}")
print(f"🏗️ Available constructs: {len(available_constructs)}")

# Create SEM dataset
if sem_variables:
    sem_data = df_clean[sem_variables].copy()
    
    # Handle categorical variables by encoding them
    from sklearn.preprocessing import LabelEncoder
    
    label_encoders = {}
    for col in sem_data.columns:
        if sem_data[col].dtype == 'object':
            le = LabelEncoder()
            sem_data[col] = le.fit_transform(sem_data[col].astype(str))
            label_encoders[col] = le
            print(f"📝 Encoded categorical variable: {col}")
    
    # Remove any remaining missing values
    sem_data = sem_data.dropna()
    
    print(f"\n✅ SEM dataset prepared: {sem_data.shape}")
    print(f"📋 Variables: {list(sem_data.columns)}")
    
else:
    print("❌ No suitable variables found for SEM analysis")## 🔬 5. Structural Equation Modeling (SEM) Analysis

Now we'll conduct the main SEM analysis. SEM is particularly suitable for this study because it allows us to:

- **Model latent constructs** that cannot be directly observed (like socioeconomic status)
- **Test complex relationships** between multiple variables simultaneously  
- **Account for measurement error** in our observed variables
- **Examine direct and indirect effects** on travel behavior

### 🎯 Our Theoretical Model:

We hypothesize that travel behavior in Khulna City is influenced by five key latent constructs:

1. **Sociodemographic**: Gender + Total_Family_Member
2. **Housing**: House_Ownership + Building_Material_Roof + Building_Material_Wall + Building_Material_Floor
3. **Economic**: Family_Income + Vehicle_Ownership
4. **Travel_Activity**: Total_Trip + Total_Cost + Total_Distance + Total_Time
5. **Accessibility**: Working_Mode + School_Mode + Shopping_Mode + Recreation_Mode# 4. Transport Mode Usage Analysis
plt.figure(figsize=(16, 10))

transport_modes = ['Working_Mode', 'School_Mode', 'Shopping_Mode', 'Recreation_Mode']
available_modes = [mode for mode in transport_modes if mode in df_clean.columns]

if available_modes:
    n_modes = len(available_modes)
    
    # Create subplots for each transport mode
    for i, mode in enumerate(available_modes, 1):
        plt.subplot(2, 2, i)
        
        mode_counts = df_clean[mode].value_counts()
        colors = sns.color_palette("deep", len(mode_counts))
        
        # Create pie chart for each mode
        wedges, texts, autotexts = plt.pie(mode_counts.values, 
                                           labels=mode_counts.index, 
                                           autopct='%1.1f%%',
                                           colors=colors, 
                                           startangle=90,
                                           textprops={'fontsize': 9, 'weight': 'bold'})
        
        # Improve text readability
        for autotext in autotexts:
            autotext.set_color('white')
            autotext.set_fontweight('bold')
        
        plt.title(f'{mode.replace("_", " ").title()} Distribution', 
                 fontsize=12, weight='bold', pad=15)
    
    plt.suptitle('Transport Mode Usage Patterns in Khulna City', 
                 fontsize=16, weight='bold', y=0.98)
    
else:
    plt.text(0.5, 0.5, 'Transport mode columns not found', 
             horizontalalignment='center', verticalalignment='center', 
             transform=plt.gca().transAxes, fontsize=12)
    plt.title('Transport Mode Usage', fontsize=14, weight='bold')

plt.tight_layout()
plt.show()# 3. House Ownership Categories
plt.figure(figsize=(12, 8))

if 'House_Ownership' in df_clean.columns:
    house_counts = df_clean['House_Ownership'].value_counts()
    
    # Create a stylish bar chart
    plt.subplot(2, 1, 1)
    colors = sns.color_palette("viridis", len(house_counts))
    bars = plt.bar(house_counts.index, house_counts.values, color=colors, 
                   edgecolor='black', linewidth=1, alpha=0.8)
    
    plt.title('House Ownership Distribution in Khulna City', fontsize=14, weight='bold', pad=20)
    plt.xlabel('Ownership Type', fontsize=12, weight='bold')
    plt.ylabel('Number of Households', fontsize=12, weight='bold')
    plt.xticks(rotation=45, ha='right')
    
    # Add value labels on bars
    for bar, value in zip(bars, house_counts.values):
        plt.text(bar.get_x() + bar.get_width()/2, bar.get_height() + max(house_counts.values) * 0.01,
                f'{value}\n({value/len(df_clean)*100:.1f}%)', 
                ha='center', va='bottom', fontweight='bold', fontsize=10)
    
    plt.grid(axis='y', alpha=0.3)
    
    # Create a donut chart for a modern look
    plt.subplot(2, 1, 2)
    wedges, texts, autotexts = plt.pie(house_counts.values, labels=house_counts.index, 
                                       autopct='%1.1f%%', colors=colors, startangle=90,
                                       textprops={'fontsize': 11, 'weight': 'bold'},
                                       pctdistance=0.85)
    
    # Create donut effect
    centre_circle = plt.Circle((0,0), 0.70, fc='white')
    fig = plt.gcf()
    fig.gca().add_artist(centre_circle)
    
    plt.title('House Ownership Proportions', fontsize=14, weight='bold', pad=20)
    
else:
    plt.text(0.5, 0.5, 'House_Ownership column not found', 
             horizontalalignment='center', verticalalignment='center', 
             transform=plt.gca().transAxes, fontsize=12)
    plt.title('House Ownership Distribution', fontsize=14, weight='bold')

plt.tight_layout()
plt.show()# 2. Vehicle Ownership Types
plt.figure(figsize=(14, 6))

if 'Vehicle_Ownership' in df_clean.columns:
    # Create a horizontal bar chart for better readability
    vehicle_counts = df_clean['Vehicle_Ownership'].value_counts()
    
    # Use a beautiful color palette
    colors = sns.color_palette("Set2", len(vehicle_counts))
    
    plt.subplot(1, 2, 1)
    bars = plt.barh(vehicle_counts.index, vehicle_counts.values, color=colors, 
                    edgecolor='black', linewidth=0.5, alpha=0.8)
    
    plt.title('Vehicle Ownership Distribution', fontsize=14, weight='bold', pad=20)
    plt.xlabel('Number of Households', fontsize=12, weight='bold')
    plt.ylabel('Vehicle Type', fontsize=12, weight='bold')
    
    # Add value labels on bars
    for i, (bar, value) in enumerate(zip(bars, vehicle_counts.values)):
        plt.text(bar.get_width() + max(vehicle_counts.values) * 0.01, bar.get_y() + bar.get_height()/2, 
                f'{value}\n({value/len(df_clean)*100:.1f}%)', 
                ha='left', va='center', fontweight='bold', fontsize=10)
    
    plt.grid(axis='x', alpha=0.3)
    
    # Create a pie chart as well
    plt.subplot(1, 2, 2)
    plt.pie(vehicle_counts.values, labels=vehicle_counts.index, autopct='%1.1f%%',
            colors=colors, startangle=90, textprops={'fontsize': 10, 'weight': 'bold'})
    plt.title('Vehicle Ownership Proportions', fontsize=14, weight='bold', pad=20)
    
else:
    plt.text(0.5, 0.5, 'Vehicle_Ownership column not found', 
             horizontalalignment='center', verticalalignment='center', 
             transform=plt.gca().transAxes, fontsize=12)
    plt.title('Vehicle Ownership Distribution', fontsize=14, weight='bold')

plt.tight_layout()
plt.show()# 1. Household Income Distribution
plt.figure(figsize=(12, 8))

# Create the histogram with enhanced styling
plt.subplot(2, 2, 1)
if 'Family_Income' in df_clean.columns:
    sns.histplot(data=df_clean, x='Family_Income', bins=30, kde=True, 
                 color='skyblue', alpha=0.7, edgecolor='black', linewidth=0.5)
    plt.title('Household Income Distribution in Khulna City', fontsize=14, weight='bold', pad=20)
    plt.xlabel('Family Income (BDT)', fontsize=12, weight='bold')
    plt.ylabel('Frequency', fontsize=12, weight='bold')
    plt.xticks(rotation=45)
    
    # Add statistical annotations
    mean_income = df_clean['Family_Income'].mean()
    median_income = df_clean['Family_Income'].median()
    plt.axvline(mean_income, color='red', linestyle='--', alpha=0.8, label=f'Mean: {mean_income:,.0f}')
    plt.axvline(median_income, color='orange', linestyle='--', alpha=0.8, label=f'Median: {median_income:,.0f}')
    plt.legend(frameon=True, fancybox=True, shadow=True)
    
    # Add grid for better readability
    plt.grid(True, alpha=0.3)
else:
    plt.text(0.5, 0.5, 'Family_Income column not found', 
             horizontalalignment='center', verticalalignment='center', 
             transform=plt.gca().transAxes, fontsize=12)
    plt.title('Household Income Distribution', fontsize=14, weight='bold')

plt.tight_layout()
plt.show()## 🎨 4. Data Visualization

Now we'll create publication-quality visualizations to explore the key patterns in our data. All figures are designed with clear labeling, appropriate color schemes, and professional formatting.# Descriptive statistics for categorical variables
if categorical_vars:
    print("📝 CATEGORICAL VARIABLES - FREQUENCY DISTRIBUTIONS")
    print("=" * 65)
    
    for var in categorical_vars:
        print(f"\n🏷️ {var.upper()}")
        print("-" * 30)
        
        # Calculate frequencies and percentages
        freq_table = df_clean[var].value_counts()
        percent_table = df_clean[var].value_counts(normalize=True) * 100
        
        # Combine into a nice table
        summary_table = pd.DataFrame({
            'Frequency': freq_table,
            'Percentage': percent_table.round(1)
        })
        
        display(summary_table)
        
        print(f"Total unique categories: {df_clean[var].nunique()}")
        print(f"Most common: {freq_table.index[0]} ({percent_table.iloc[0]:.1f}%)")
        print()# Descriptive statistics for numeric variables
if numeric_vars:
    print("🔢 NUMERIC VARIABLES - DESCRIPTIVE STATISTICS")
    print("=" * 60)
    
    # Calculate comprehensive descriptive statistics
    desc_stats = df_clean[numeric_vars].describe()
    
    # Add additional statistics
    additional_stats = pd.DataFrame({
        var: {
            'mode': df_clean[var].mode().iloc[0] if not df_clean[var].mode().empty else np.nan,
            'variance': df_clean[var].var(),
            'skewness': stats.skew(df_clean[var]),
            'kurtosis': stats.kurtosis(df_clean[var])
        } for var in numeric_vars
    }).T
    
    # Combine statistics
    full_stats = pd.concat([desc_stats.T, additional_stats], axis=1)
    
    # Round for better readability
    full_stats = full_stats.round(3)
    
    display(full_stats)
    
    # Create a summary table for key insights
    print("\n📋 KEY INSIGHTS FROM NUMERIC VARIABLES")
    print("=" * 45)
    
    for var in numeric_vars:
        mean_val = df_clean[var].mean()
        median_val = df_clean[var].median()
        std_val = df_clean[var].std()
        cv = (std_val / mean_val) * 100 if mean_val != 0 else 0
        
        print(f"• {var}:")
        print(f"  - Mean: {mean_val:.2f}, Median: {median_val:.2f}, Std: {std_val:.2f}")
        print(f"  - Coefficient of Variation: {cv:.1f}%")
        print(f"  - Range: {df_clean[var].min():.2f} - {df_clean[var].max():.2f}")
        print()# Define the variables for analysis
analysis_variables = [
    'Gender', 'Total_Family_Member', 'House_Ownership', 
    'Building_Material_Roof', 'Building_Material_Wall', 'Building_Material_Floor',
    'Family_Income', 'Vehicle_Ownership', 'Total_Trip', 
    'Total_Cost', 'Total_Distance', 'Total_Time',
    'Working_Mode', 'School_Mode', 'Shopping_Mode', 'Recreation_Mode'
]

# Filter variables that exist in the dataset
available_variables = [var for var in analysis_variables if var in df_clean.columns]

print("📈 DESCRIPTIVE STATISTICS SUMMARY")
print("=" * 50)
print(f"Variables available for analysis: {len(available_variables)}")
print(f"Variables: {available_variables}")

# Separate numeric and categorical variables
numeric_vars = df_clean[available_variables].select_dtypes(include=[np.number]).columns.tolist()
categorical_vars = df_clean[available_variables].select_dtypes(include=['object']).columns.tolist()

print(f"\n🔢 Numeric variables ({len(numeric_vars)}): {numeric_vars}")
print(f"📝 Categorical variables ({len(categorical_vars)}): {categorical_vars}")## 📊 3. Descriptive Statistics Analysis

Let's examine the descriptive statistics for all key variables in our study. This will help us understand the distribution and characteristics of our sample.# Data cleaning and preprocessing
print("🧹 DATA CLEANING PROCESS")
print("=" * 40)

# Create a copy for cleaning
df_clean = df.copy()

# Remove rows with missing values (if any)
initial_rows = len(df_clean)
df_clean = df_clean.dropna()
final_rows = len(df_clean)
print(f"📊 Rows before cleaning: {initial_rows}")
print(f"📊 Rows after cleaning: {final_rows}")
print(f"🗑️ Rows removed: {initial_rows - final_rows}")

# Ensure numeric columns are properly typed
numeric_columns = [
    'Total_Family_Member', 'Family_Income', 'Total_Trip', 
    'Total_Cost', 'Total_Distance', 'Total_Time'
]

for col in numeric_columns:
    if col in df_clean.columns:
        df_clean[col] = pd.to_numeric(df_clean[col], errors='coerce')

# Remove any rows that became NaN after numeric conversion
df_clean = df_clean.dropna()

print(f"✅ Final dataset shape: {df_clean.shape}")
print(f"✅ Data cleaning completed successfully!")# Display basic information about the dataset
print("🔍 DATASET OVERVIEW")
print("=" * 50)
print(f"Number of observations: {len(df)}")
print(f"Number of variables: {len(df.columns)}")
print(f"Memory usage: {df.memory_usage(deep=True).sum() / 1024**2:.2f} MB")

print("\n📈 DATA TYPES")
print("=" * 30)
print(df.dtypes)

print("\n🧹 MISSING VALUES")
print("=" * 30)
missing_values = df.isnull().sum()
missing_percent = (missing_values / len(df)) * 100
missing_df = pd.DataFrame({
    'Missing Count': missing_values,
    'Missing Percentage': missing_percent
})
print(missing_df[missing_df['Missing Count'] > 0])

print("\n👀 FIRST FEW ROWS")
print("=" * 30)
df.head()# Load the dataset
# For Google Colab, upload the file using the file upload widget
from google.colab import files

print("📁 Please upload your 'urban_travel_data.csv' file:")
uploaded = files.upload()

# Load the data
df = pd.read_csv('urban_travel_data.csv')

print(f"✅ Dataset loaded successfully!")
print(f"📊 Dataset shape: {df.shape}")
print(f"📋 Columns: {list(df.columns)}")## 📂 2. Data Loading and Initial Exploration

We'll load the urban travel dataset and perform initial data exploration to understand the structure and quality of our data.# Import essential libraries
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import warnings
from scipy import stats
import semopy
from semopy import Model
from semopy.inspector import inspect
import graphviz
from sklearn.preprocessing import StandardScaler

# Configure plotting settings for publication-quality figures
sns.set_theme(style="whitegrid", font_scale=1.3, palette="muted")
plt.rcParams.update({
    'font.size': 12,
    'axes.titlesize': 14,
    'axes.labelsize': 12,
    'xtick.labelsize': 11,
    'ytick.labelsize': 11,
    'legend.fontsize': 11,
    'figure.titlesize': 16,
    'axes.titleweight': 'bold'
})

# Suppress warnings for cleaner output
warnings.filterwarnings('ignore')

# Set random seed for reproducibility
np.random.seed(42)

print("✅ All packages imported successfully!")
print(f"📊 Seaborn version: {sns.__version__}")
print(f"🔧 Semopy version: {semopy.__version__}")# Install required packages for Google Colab
!pip install semopy graphviz seaborn matplotlib pandas numpy scikit-learn --quiet
!apt-get install -y graphviz --quiet## 📦 1. Package Installation and Setup

First, we'll install and import all necessary packages for our analysis.# 🏙️ Urban Travel Behavior Analysis Using SEM – A Case Study of Khulna City

---

## 📋 Research Overview

This notebook presents a comprehensive analysis of urban travel behavior in Khulna City, Bangladesh, using **Structural Equation Modeling (SEM)**. We examine how sociodemographic factors, housing characteristics, economic status, and accessibility influence travel patterns in this rapidly growing urban center.

### 🎯 Research Objectives:
- Analyze descriptive patterns in urban mobility data
- Model complex relationships between latent constructs affecting travel behavior
- Provide insights for urban transport planning and policy development

### 🛠️ Methodology:
- **Data Source**: Urban travel survey from Khulna City
- **Statistical Method**: Structural Equation Modeling (SEM)
- **Tools**: Python (semopy, seaborn, matplotlib)

---